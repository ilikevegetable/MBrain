{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "julian-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import scipy.io as scio"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You have to donwload the dataset and define the path below.",
   "id": "44dc7488b9d0aadb"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "temporal-finish",
   "metadata": {},
   "outputs": [],
   "source": "file_path = '/data/SEED_EEG/Preprocessed_EEG/'"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "interim-isolation",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10_20131130.mat',\n",
       " '10_20131204.mat',\n",
       " '10_20131211.mat',\n",
       " '11_20140618.mat',\n",
       " '11_20140625.mat',\n",
       " '11_20140630.mat',\n",
       " '12_20131127.mat',\n",
       " '12_20131201.mat',\n",
       " '12_20131207.mat',\n",
       " '13_20140527.mat',\n",
       " '13_20140603.mat',\n",
       " '13_20140610.mat',\n",
       " '14_20140601.mat',\n",
       " '14_20140615.mat',\n",
       " '14_20140627.mat',\n",
       " '15_20130709.mat',\n",
       " '15_20131016.mat',\n",
       " '15_20131105.mat',\n",
       " '1_20131027.mat',\n",
       " '1_20131030.mat',\n",
       " '1_20131107.mat',\n",
       " '2_20140404.mat',\n",
       " '2_20140413.mat',\n",
       " '2_20140419.mat',\n",
       " '3_20140603.mat',\n",
       " '3_20140611.mat',\n",
       " '3_20140629.mat',\n",
       " '4_20140621.mat',\n",
       " '4_20140702.mat',\n",
       " '4_20140705.mat',\n",
       " '5_20140411.mat',\n",
       " '5_20140418.mat',\n",
       " '5_20140506.mat',\n",
       " '6_20130712.mat',\n",
       " '6_20131016.mat',\n",
       " '6_20131113.mat',\n",
       " '7_20131027.mat',\n",
       " '7_20131030.mat',\n",
       " '7_20131106.mat',\n",
       " '8_20140511.mat',\n",
       " '8_20140514.mat',\n",
       " '8_20140521.mat',\n",
       " '9_20140620.mat',\n",
       " '9_20140627.mat',\n",
       " '9_20140704.mat',\n",
       " 'label.mat']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_file = []\n",
    "for file in os.listdir(file_path):\n",
    "    if file[-3:] == 'mat':\n",
    "        all_file.append(file)\n",
    "all_file.sort()\n",
    "all_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "grave-villa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 4, 6, 7, 8, 10, 12, 14]\n",
      "[2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# 1. Divide the dataset into train/valid/test with ratio of 3:1:1\n",
    "# train_idx = [2, 4, 5, 7, 8, 9, 11, 13, 15]\n",
    "# valid_idx = [1, 6, 14]\n",
    "# test_idx = [3, 10, 12]\n",
    "\n",
    "train_idx = list(np.random.choice(15, 9, replace=False))\n",
    "valid_idx = list(np.random.choice(6, 3, replace=False))\n",
    "train_idx.sort()\n",
    "valid_idx.sort()\n",
    "\n",
    "print(train_idx)\n",
    "print(valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "innovative-liquid",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = [2, 4, 5, 7, 8, 10, 11, 12, 15]\n",
    "valid_idx = [1, 3, 14]\n",
    "test_idx = [6, 9, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "loving-jaguar",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = []\n",
    "valid_file = []\n",
    "test_file = []\n",
    "\n",
    "for file in all_file:\n",
    "    if file == 'label.mat':\n",
    "        continue\n",
    "    patient_idx = int(file.split('_')[0])\n",
    "    if patient_idx in train_idx:\n",
    "        train_file.append(file)\n",
    "    elif patient_idx in valid_idx:\n",
    "        valid_file.append(file)\n",
    "    else:\n",
    "        test_file.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "super-studio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "reported-generator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, -1, -1, 0, 1, -1, 0, 1, 1, 0, -1, 0, 1, -1]\n"
     ]
    }
   ],
   "source": [
    "label = scio.loadmat(os.path.join(file_path, 'label.mat'))['label']\n",
    "label = list(label[0])\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "patient-begin",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "valid_data = []\n",
    "test_data = []\n",
    "train_label = []\n",
    "valid_label = []\n",
    "test_label = []\n",
    "\n",
    "segment_length = 12 * 200 * 2\n",
    "idx = np.arange(1, 401, 2)\n",
    "\n",
    "for file in train_file:\n",
    "    data = scio.loadmat(os.path.join(file_path, file))\n",
    "    for key in data.keys():\n",
    "        if key[:2] == '__':\n",
    "            continue\n",
    "        film_idx = int(key.split('_')[1][3:])\n",
    "        # range from 1-15\n",
    "        film_label = label[film_idx-1] + 1\n",
    "        film_data = data[key]\n",
    "        segment_num = film_data.shape[1] // segment_length\n",
    "        filter_data = film_data[:,:segment_num*segment_length].reshape(62, segment_num, 12, 400)\n",
    "        filter_data = filter_data[:,:,:,idx]\n",
    "        \n",
    "        filter_data = filter_data.transpose((1,0,2,3))\n",
    "        filter_data = filter_data.transpose((0,2,1,3))\n",
    "        filter_label = np.full((filter_data.shape[0], 1), film_label, dtype=np.float32)\n",
    "        \n",
    "        train_data.append(filter_data)\n",
    "        train_label.append(filter_label)\n",
    "    \n",
    "for file in valid_file:\n",
    "    data = scio.loadmat(os.path.join(file_path, file))\n",
    "    for key in data.keys():\n",
    "        if key[:2] == '__':\n",
    "            continue\n",
    "        film_idx = int(key.split('_')[1][3:])\n",
    "        # range from 1-15\n",
    "        film_label = label[film_idx-1] + 1\n",
    "        film_data = data[key]\n",
    "        segment_num = film_data.shape[1] // segment_length\n",
    "        filter_data = film_data[:,:segment_num*segment_length].reshape(62, segment_num, 12, 400)\n",
    "        filter_data = filter_data[:,:,:,idx]\n",
    "        \n",
    "        filter_data = filter_data.transpose((1,0,2,3))\n",
    "        filter_data = filter_data.transpose((0,2,1,3))\n",
    "        filter_label = np.full((filter_data.shape[0], 1), film_label, dtype=np.float32)\n",
    "        \n",
    "        valid_data.append(filter_data)\n",
    "        valid_label.append(filter_label)\n",
    "        \n",
    "\n",
    "for file in test_file:\n",
    "    data = scio.loadmat(os.path.join(file_path, file))\n",
    "    for key in data.keys():\n",
    "        if key[:2] == '__':\n",
    "            continue\n",
    "        film_idx = int(key.split('_')[1][3:])\n",
    "        # range from 1-15\n",
    "        film_label = label[film_idx-1] + 1\n",
    "        film_data = data[key]\n",
    "        segment_num = film_data.shape[1] // segment_length\n",
    "        filter_data = film_data[:,:segment_num*segment_length].reshape(62, segment_num, 12, 400)\n",
    "        filter_data = filter_data[:,:,:,idx]\n",
    "        \n",
    "        filter_data = filter_data.transpose((1,0,2,3))\n",
    "        filter_data = filter_data.transpose((0,2,1,3))\n",
    "        filter_label = np.full((filter_data.shape[0], 1), film_label, dtype=np.float32)\n",
    "        \n",
    "        test_data.append(filter_data)\n",
    "        test_label.append(filter_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "documentary-clarity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3564, 12, 62, 200) (3564, 1)\n",
      "(1188, 12, 62, 200) (1188, 1)\n",
      "(1188, 12, 62, 200) (1188, 1)\n"
     ]
    }
   ],
   "source": [
    "train_data = np.concatenate(train_data, axis=0)\n",
    "train_label = np.concatenate(train_label, axis=0)\n",
    "\n",
    "valid_data = np.concatenate(valid_data, axis=0)\n",
    "valid_label = np.concatenate(valid_label, axis=0)\n",
    "\n",
    "test_data = np.concatenate(test_data, axis=0)\n",
    "test_label = np.concatenate(test_label, axis=0)\n",
    "\n",
    "print(train_data.shape, train_label.shape)\n",
    "print(valid_data.shape, valid_label.shape)\n",
    "print(test_data.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "german-witness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3564, 12, 62, 200) (3564, 1)\n",
      "(1188, 12, 62, 200) (1188, 1)\n",
      "(1188, 12, 62, 200) (1188, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape, train_label.shape)\n",
    "print(valid_data.shape, valid_label.shape)\n",
    "print(test_data.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-nigeria",
   "metadata": {},
   "source": [
    "## SSL:\n",
    "sample 3500 segments from train_data, 80% for training and 20% for validation\n",
    "1. 2800 * 12 * 62 * 200\n",
    "2. 700 * 12 * 62 * 200\n",
    "\n",
    "## Downstream:\n",
    "sample another 2000 segments from train_data\n",
    "\n",
    "sample 500 segments from valid_data\n",
    "\n",
    "use whole test_data for test set\n",
    "1. 2000 * 12 * 62 * 200\n",
    "2. 500 * 12 * 62 * 200\n",
    "3. 1188 * 12 * 62 * 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "special-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl_idx = np.random.choice(train_data.shape[0], 3500, replace=False)\n",
    "ssl_train_idx = ssl_idx[:2800]\n",
    "ssl_val_idx = ssl_idx[-700:]\n",
    "\n",
    "train_idx = np.random.choice(train_data.shape[0], 2000, replace=False)\n",
    "val_idx = np.random.choice(valid_data.shape[0], 500, replace=False)\n",
    "test_idx = np.random.choice(test_data.shape[0], test_data.shape[0], replace=False)\n",
    "# shuffle test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "juvenile-arabic",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl_train_data = train_data[ssl_train_idx]\n",
    "ssl_val_data = train_data[ssl_val_idx]\n",
    "\n",
    "train_data = train_data[train_idx]\n",
    "train_label = train_label[train_idx]\n",
    "\n",
    "val_data = valid_data[val_idx]\n",
    "val_label = valid_label[val_idx]\n",
    "\n",
    "test_data = test_data[test_idx]\n",
    "test_label = test_label[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "foster-undergraduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/data/EEG_Emotion_Recognition/'\n",
    "\n",
    "np.savez_compressed(os.path.join(save_dir, 'ssl_train_data.npz'), ssl_train_data)\n",
    "np.savez_compressed(os.path.join(save_dir, 'ssl_val_data.npz'), ssl_val_data)\n",
    "\n",
    "np.savez_compressed(os.path.join(save_dir, 'train_data.npz'), train_data)\n",
    "np.savez_compressed(os.path.join(save_dir, 'train_label.npz'), train_label)\n",
    "\n",
    "np.savez_compressed(os.path.join(save_dir, 'val_data.npz'), val_data)\n",
    "np.savez_compressed(os.path.join(save_dir, 'val_label.npz'), val_label)\n",
    "\n",
    "np.savez_compressed(os.path.join(save_dir, 'test_data.npz'), test_data)\n",
    "np.savez_compressed(os.path.join(save_dir, 'test_label.npz'), test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "optical-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_2 = '/data/EEG_Emotion_Recognition/index/'\n",
    "\n",
    "np.savez_compressed(os.path.join(save_dir_2, 'ssl_train_idx.npz'), ssl_train_idx)\n",
    "np.savez_compressed(os.path.join(save_dir_2, 'ssl_val_idx.npz'), ssl_val_idx)\n",
    "np.savez_compressed(os.path.join(save_dir_2, 'train_idx.npz'), train_idx)\n",
    "np.savez_compressed(os.path.join(save_dir_2, 'val_idx.npz'), val_idx)\n",
    "np.savez_compressed(os.path.join(save_dir_2, 'test_idx.npz'), test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-ensemble",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-entrepreneur",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-dream",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-squad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
